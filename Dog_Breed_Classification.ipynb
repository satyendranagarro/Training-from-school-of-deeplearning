{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2578051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.2.2->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.17.2-cp312-cp312-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.17.2\n"
     ]
    }
   ],
   "source": [
    "! pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a081c",
   "metadata": {},
   "source": [
    "1.  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36895f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0b8ac",
   "metadata": {},
   "source": [
    "2. Paths and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d244db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Adjust paths to your dataset\n",
    "train_path = \"/kaggle/input/dog-breed-identification/train\"\n",
    "val_path = \"/kaggle/input/dog-breed-identification/val\"\n",
    "test_path = \"/kaggle/input/dog-breed-identification/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705dceeb",
   "metadata": {},
   "source": [
    "3. Transforms and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851f3c84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      2\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)),\n\u001b[1;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m])\n\u001b[1;32m      5\u001b[0m ])\n\u001b[0;32m----> 7\u001b[0m train_data \u001b[38;5;241m=\u001b[39m ImageFolder(train_path, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m      8\u001b[0m val_data \u001b[38;5;241m=\u001b[39m ImageFolder(val_path, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m      9\u001b[0m test_data \u001b[38;5;241m=\u001b[39m ImageFolder(test_path, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_path' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_data = ImageFolder(train_path, transform=transform)\n",
    "val_data = ImageFolder(val_path, transform=transform)\n",
    "test_data = ImageFolder(test_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "num_classes = len(train_data.classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf685cb",
   "metadata": {},
   "source": [
    "4. Simple CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DogCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = DogCNN(num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2fb8c",
   "metadata": {},
   "source": [
    " 5. Loss, Optimizer, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e867128",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b275e6",
   "metadata": {},
   "source": [
    "6. Train + Validate Loop (with Graph Stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72827505",
   "metadata": {},
   "source": [
    "7. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92274802",
   "metadata": {},
   "source": [
    "8. Plot Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
